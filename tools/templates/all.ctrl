# Each line that starts with '#' is a comment/description of the variable above the comment
# Each line that starts with '*' belongs to a section heading

****** Job Resource Configuration

job_name=testing
# alphabetic characters (i.e. letters from a-z or A-Z)
# Used to describe distinct runs (using the same name will
# overwrite data if using S3!)

threads_per_docking=1
# How many threads should be used for each docking program. 

threads_to_use=8
# This sets how many processes the main execution loop should be using
# to process. This is generally 2x the number of vCPUs or hyperthreads
# available on the system it is being run on

program_timeout=90
# How many seconds to wait for each ligand to be processed by a program


************************************************
** Batch system configuration
************************************************

batchsystem=awsbatch
# Possible values: awsbatch, slurm

****** AWS Batch Options (if batchsystem=awsbatch)

### To use AWS Batch you must first complete the steps outlined 
### in the user guide for AWS Batch 

aws_batch_prefix=vf
# Prefix for the name of the AWS Batch queues. This is normally 'vf'
# if you used the CloudFormation template

aws_batch_number_of_queues=2
# Should be set to the number of queues that are setup for AWS Batch. 
# Generally this number is 2 unless you have a large-scale (100K+ vCPUs)
# setup

aws_batch_jobdef=vf-jobdef-vfvs
# Generally this is [aws_batch_prefix]-jobdef-vfvs
# (e.g. if aws_batch_prefix=vf, then aws_batch_jobdef=vf-jobdef-vfvs

aws_batch_array_job_size=200
# Target for the number of jobs that should be in a single array job for AWS Batch.

aws_ecr_repository_name=vf-vfvs-ecr
# Set it to the name of the Elastic Container Registry (ECR) 
# repository (e.g. vf-vfvs-ecr) in your AWS account
# (If you used the template it is generally vf-vfvs-ecr)

aws_region=us-east-1
# Set to the AWS location code where you are running AWS Batch
# (e.g. us-east-1 for North America, Northern Virginia)

aws_batch_subjob_vcpus=8
# Set to the number of vCPUs that should be launched per subjob. 
# 'threads_to_use' above should be >= to this value. 

aws_batch_subjob_memory=15000
# Memory per subjob to setup for the container in MB

aws_batch_subjob_timeout=10800
# Maximum amount of time (in seconds) that a single AWS Batch job should 
# ever run before being terminated.

****** Slurm Options (if batchsystem=slurm)

slurm_template=./templates/template1.slurm.sh
# Template for the slurm job
# Additional slurm attributes can be added directly to this
# template file if they are not available as pass throughs from
# VFVS

slurm_array_job_throttle=100
# Maximum number of jobs running within a single slurm array job

slurm_partition=partition
# Partition to submit the job

slurm_cpus=18
# Number of CPUs that are being used

slurm_array_job_size=100
# Maximum number of concurrent jobs from a single array job
# that should be run


************************************************
** Storage configuration
************************************************

job_storage_mode=s3
# This mode determines where data is retrieved and stored from as part of 
# VFVS. Valid modes:
#   * s3: Job data is stored on S3 object store, which is the required
#         mode if using AWS Batch. Items under the "S3 Object Store" 
#         heading in the configuration are required if this mode is used
#   * sharedfs: This mode requires that all running jobs have access to the
#         same shared filesystem that will allow for both input and output
#         of data. This is required if using Slurm


data_collection_addressing_mode=metatranche
# If input is placed with the hash addressing mode, then use 'hash'.
# otherwise use "metatranche" for the classic addressing mode

data_collection_identifier=
# This is only used if object_store_data_collection_addressing_mode=hash
# Generally this is the dataset name (e.g. Enamine_REAL_Space_2021q12)

job_addressing_mode=metatranche
# If job output is to be placed with the hash addressing mode, then use 'hash'.
# otherwise use "metatranche" for the classic addressing mode


****** Object Store Settings (S3)

object_store_job_bucket=
# Bucket name for the job data (output and job files)

object_store_job_prefix=jobs
# Where to place job-specific data. This includes where VirtualFlow will place 
# the input data needed for jobs as well as the output files. 
#
# Data be be placed:
# if object_store_job_addressing_mode=hash
#	in object_store_job_prefix/XX/YY/<job_letter>
#       (where 'XX', and 'YY' are hash values that will vary for 
#        different files)
# else
# 	in object_store_job_prefix/<job_letter>

object_store_data_bucket=
# Bucket name for the input collection data (often the same as the job one)

object_store_data_collection_prefix=
# Prefix used within the object store to address the collections




****** Shared Filesystem Settings

collection_folder=/home/ec2-user/collections
# Path to where the collection file are stored
#  * This is used when job_storage_mode=sharedfs or
#    when the uploader helper script is being used
#
# Slash at the end is not required (optional)
# Either pathname is required w.r.t. the folder tools/
#     or absolute path (e.g. /home/vfuser/collections)


************************************************
** Run configuration
************************************************

****** Output information

summary_formats=parquet
# Format for summary files that are generated with the score data.
# Supported values:
#  * txt.gz (space delimited files)
#  * parquet
# Multiple formats can be generated by placing a comma
# (e.g. summary_formats=parquet,txt.gz)

print_smi_in_summary=1
# Whether or not the SMILES string should be printed in 
# the summary file (works for pdbqt and mol2)
# Supported values:
#  * 0 : Do not provide
#  * 1 : SMILES string in the summary file


****** Workflow Options

ligands_todo_per_queue=1000
# Used as a limit of ligands for the to-do lists. 
# A reasonable number for this is generally 1000. The length of time
#     to process will depend on the docking scenarios run

ligand_library_format=pdbqt
# Supported values:
#  * pdbqt
#  * mol2
# This value is case sensitive
# All AutoDock based docking programs require the library to
#     be in the pdbqt format.
# When the docking program PLANTS is used, both libraries in the
#     pdbqt and the mol2 format are supported.

tempdir_default=/dev/shm
# The directory which is used for the temporary workflow files which need a normal performance
# Is normally a local SSD or HDD. A temporary directory will be created underneath this dir


tempdir_fast=/dev/shm
# The directory which is used for the temporary workflow files which need a fast perfomance
# Should be a a local ram filesystem/ramdisk. A temporary directory will be created underneath this dir

****** Virtual Screening Options

docking_scenario_names=qvina02_rigid_receptor1
# Names for the docking scenarios, separated by colons
# Each docking scenario has one value. Multiple docking scenarios/names have
#    to be separated by colons ":" and without spaces
#
# Example: docking_scenario_names=receptor1_vina_rigid:receptor1_smina_flexible
# The docking scenario names are used for the folder names in which the output files are stored

docking_scenario_programs=qvina02
# For each docking scenario name, a docking program has to be specified
# Possible values: qvina02, qvina_w, vina, smina_rigid, smina_flexible, gwovina, adfr
# Values have to be separated by colons ":" and without spaces, e.g: docking_scenario_programs=vina:smina
# smina_rigid has to be used for rigid docking with smina, while smine_flexible for flexible receptor docking

docking_scenario_replicas=1
# Series of integers separated by colons ":"
# The number of values has to equal the number of docking programs
#     specified in the variable "docking_programs"
# The values are in the same order as the docking programs specified in the
#     variable "docking_scenario_programs
#     e.g.: docking_scenario_replicas=1:1
# possible range: 1-99999 per field/docking program
# The docking scenario is comprised of all the docking types and their replicas

docking_scenario_basefolder=../input-files
# Relative path to tools directory
# Base directory for where the docking scenarios are held. Nothing other 
# than the required files for the docking scenario should be placed here

docking_scenario_inputfolders=qvina02_rigid_receptor1
# folder names inside 'docking_scenario_basefolder' 
# In each input folder must be the file config.txt which is used by the
#     docking program to specify its options
# If other input files are required by the docking type, usually
#     specified in the config.txt file, they have to be in the same folder

