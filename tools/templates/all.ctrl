****** Job Resource Configuration 

job_name=testing
# alphabetic characters (i.e. letters from a-z or A-Z)
# Used to describe distinct runs (using the same name will
# overwrite data if using S3!)

threads_per_docking=1
# How many threads should be used for each docking program. 

threads_to_use=16
# This sets how many processes the main execution loop should be using
# to process. This is generally 2x the number of vCPUs or hyperthreads
# available on the system it is being run on

program_timeout=90
# How many seconds to wait for each ligand to be processed by a program


************************************************
** Batch system configuration
************************************************

batchsystem=awsbatch
# Possible values: awsbatch, slurm

****** AWS Batch Options (if batchsystem=awsbatch)

### To use AWS Batch you must first complete the steps outlined 
### in the user guide for AWS Batch 

aws_batch_prefix=vf
# Prefix for the name of the AWS Batch queues. This is normally 'vf'
# if you used the CloudFormation template

aws_batch_number_of_queues=2
# Should be set to the number of queues that are setup for AWS Batch. 
# Generally this number is 2 unless you have a large-scale (100K+ vCPUs)
# setup

aws_batch_jobdef=vf-jobdef-vfvs
# Generally this is [aws_batch_prefix]-jobdef-vfvs
# (e.g. if aws_batch_prefix=vf, then aws_batch_jobdef=vf-jobdef-vfvs

aws_batch_array_job_size=200
# Target for the number of jobs that should be in a single array job for AWS Batch.

aws_ecr_repository_name=vf-vfvs-ecr
# Set it to the name of the Elastic Container Registry (ECR) 
# repository (e.g. vf-vfvs-ecr) in your AWS account
# (If you used the template it is generally vf-vfvs-ecr)

aws_region=us-east-2
# Set to the AWS location code where you are running AWS Batch
# (e.g. us-east-1 for North America, Northern Virginia)

aws_batch_subjob_vcpus=8
# Set to the number of vCPUs that should be launched per subjob. 
# 'threads_to_use' above should be >= to this value. 

aws_batch_subjob_memory=15000
# Memory per subjob to setup for the container in MB

aws_batch_subjob_timeout=10800
# Maximum amount of time (in seconds) that a single AWS Batch job should 
# ever run before being terminated.

****** Slurm Options (if batchsystem=slurm)

slurm_template=./templates/template1.slurm.sh
# Template for the slurm job
# Additional slurm attributes can be added directly to this
# template file if they are not available as pass throughs from
# VFVS

slurm_array_job_throttle=100
# Maximum number of jobs running within a single slurm array job

slurm_partition=partition
# Partition to submit the job

slurm_cpus=18
# Number of CPUs that are being used

slurm_array_job_size=100
# Maximum number of concurrent jobs from a single array job
# that should be run


************************************************
** Storage configuration
************************************************

job_storage_mode=s3
# This mode determines where data is retrieved and stored from as part of 
# VFVS. Valid modes:
#   * s3: Job data is stored on S3 object store, which is the required
#         mode if using AWS Batch. Items under the "S3 Object Store" 
#         heading in the configuration are required if this mode is used
#   * sharedfs: This mode requires that all running jobs have access to the
#         same shared filesystem that will allow for both input and output
#         of data. This is required if using Slurm


data_collection_addressing_mode=metatranche
# If input is placed with the hash addressing mode, then use 'hash'.
# otherwise use "metatranche" for the classic addressing mode

data_collection_identifier=
# This is only used if object_store_data_collection_addressing_mode=hash
# Generally this is the dataset name (e.g. Enamine_REAL_Space_2021q12)

job_addressing_mode=metatranche
# If job output is to be placed with the hash addressing mode, then use 'hash'.
# otherwise use "metatranche" for the classic addressing mode


****** Object Store Settings (S3)

object_store_job_bucket=
# Bucket name for the job data (output and job files)

object_store_job_prefix=jobs
# Where to place job-specific data. This includes where VirtualFlow will place 
# the input data needed for jobs as well as the output files. 
#
# Data be be placed:
# if object_store_job_addressing_mode=hash
#	in object_store_job_prefix/XX/YY/<job_letter>
#       (where 'XX', and 'YY' are hash values that will vary for 
#        different files)
# else
# 	in object_store_job_prefix/<job_letter>

object_store_data_bucket=
# Bucket name for the input collection data (often the same as the job one)

object_store_data_collection_prefix=
# Prefix used within the object store to address the collections




****** Shared Filesystem Settings

collection_folder=/home/ec2-user/collections
# Path to where the collection file (ready-to-dock ligands/docking requirement files) 
# are stored
#  * This is used when job_storage_mode=sharedfs or
#    when the uploader helper script is being used
#
# Slash at the end is not required (optional)
# Either pathname is required w.r.t. the folder tools/
#     or absolute path (e.g. /home/vfuser/collections)


************************************************
** Run configuration
************************************************

****** Output information

summary_formats=parquet,csv.gz
# Format for summary files that are generated with the score data.
# Supported values:
#  * csv.gz (comma delimited files)
#  * parquet
# Multiple formats can be generated by placing a comma
# (e.g. summary_formats=parquet,csv.gz)

print_attrs_in_summary=smi,heavy_atom_count
# Attributes that should be printed in the summary file
# valid for pdbqt, pdb, mol2, and sdf files that include
# the attributes
#
# multiple attributes should be comma-delimited:
# e.g. smi,heavy_atom_count
#
# Supported values:
#  *     : blank -- no attributes
#  * smi : SMILES string
#  * heavy_atom_count : heavy atom count


****** Workflow Options


collection_list_type=standard
# This determines the input format for the list of collections that 
# should be processed as part of the workflow. There are two different
# available formats:
#  * standard : This is the format that has been traditionally used
#       with VirtualFlow. The format is: <collection_key> <number_of_ligands_in_collection>
#       e.g. AABCDAA_0000 10000
#  * csv_collection_key_ligand : This format allows selecting a specified subset of ligands
#       within a collection to be run. The format is:
#
#       collection_name,collection_number,ligand
#       CADBCCCDCCAACCAAAA,0007276,m_275592____15865294____14140940____14133308_S3_T2
#       CADBCCCDCCAACCAAAA,0007276,m_275592____15865294____14139142____14132874_S1_T0
#
#       The header line with "collection_name,..." is required for this format.
#

collection_list_file=templates/todo.all
# Path to the file that contains the collection data on what should be 
# processed as part of the workflow

dockings_per_subjob=1000
# Used as how many dockings should be processed per subjob. In general,
# a subjob should take about 20 min to an hour for efficiency. 
# A reasonable number for this is generally 1000. The length of time
# to process will depend on the docking scenarios run

ligand_library_format=pdbqt
# Supported values:
#  * pdbqt, mol2, pdb, sdf, etc. (all output formats generated by VFLP are supported)
# This value is case sensitive
# All AutoDock based docking programs require the library to
#     be in the pdbqt format.
# For the selected docking program, the user is required to set
#     this to a compatible value. Eg: for plants, set this to mol2

tempdir_default=/dev/shm
# The directory which is used for the temporary workflow files. Typically this is 
# /dev/shm, which is an in-memory filesystem location.


****** Virtual Screening Options

docking_scenario_names=qvina02_rigid_receptor1
# Names for the docking scenarios, separated by colons
# Each docking scenario has one value. Multiple docking scenarios/names have
#    to be separated by colons ":" and without spaces
#
# Example: docking_scenario_names=receptor1_vina_rigid:receptor1_smina_flexible
# The docking scenario names are used for the folder names in which the output files are stored

docking_scenario_programs=qvina02
# For each docking scenario name, a docking program has to be specified
# Possible values: qvina02, qvina_w, vina, smina_rigid, smina_flexible, gwovina, adfr, AutodockVina_1.2, AutodockZN
#                  gnina, rDock, M-Dock, MCDock, LigandFit, ledock, gold, iGemDock, idock, GalaxyDock3, autodock_gpu
#                  autodock_cpu, autodock_koto, RLDock, PSOVina, LightDock, FitDock, Molegro, rosetta-ligand, SEED
#                  MpSDockZN, AutodockVina_1.1.2, CovDock, dock6
# Values have to be separated by colons ":" and without spaces, e.g: docking_scenario_programs=vina:smina
# smina_rigid has to be used for rigid docking with smina, while smine_flexible for flexible receptor docking

docking_scenario_replicas=1
# Series of integers separated by colons ":"
# The number of values has to equal the number of docking programs
#     specified in the variable "docking_programs"
# The values are in the same order as the docking programs specified in the
#     variable "docking_scenario_programs
#     e.g.: docking_scenario_replicas=1:1
# possible range: 1-99999 per field/docking program
# The docking scenario is comprised of all the docking types and their replicas


docking_scenario_basefolder=../input-files
# Relative path to tools directory
# Base directory for where the docking scenarios are held. Nothing other 
# than the required files for the docking scenario should be placed here

docking_scenario_inputfolders=qvina02_rigid_receptor1
# folder names inside 'docking_scenario_basefolder' 
# In each input folder must be the file config.txt which is used by the
#     docking program to specify its options
# If other input files are required by the docking type, usually
#     specified in the config.txt file, they have to be in the same folder

sensor_screen_mode=0
# Run a subset of the ligands included in the library. Only functional
# if library supports the sensor screen functionality with .listing
# file inside the collection
#
# This is also only supported when collection_list_type=standard
#
# Supported values:
#   0 : Turned off, run all ligands within each collection
#   1 : Turned on, run only `sensor_screen_count` number of ligands per
#       individual collection

sensor_screen_count=1
# Number of ligands from a given collection/tranche that should
# be screened from a sensor screen enabled collection (with .listing file)
#
# Only used if sensor_screen_mode=1

run_atom_check=1
# Remove ligands when certain atoms B, Si, Sn are found

************************************************
** Query configuration
************************************************

****** AWS Athena information
# AWS Athena will allow ranking of ligand output
# using the vfvs_get_top_results.py script

athena_s3_location=
# Location of S3 data for where AWS Athena queries should
# be output
#
# Must be in format of s3://bucket_name/prefix
# e.g. s3://sample-bucket/athena/

